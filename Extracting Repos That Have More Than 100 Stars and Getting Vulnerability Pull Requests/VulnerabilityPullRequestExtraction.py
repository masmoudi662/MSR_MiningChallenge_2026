import re
import os
import pandas as pd

# === Load ===
pr_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pull_request.parquet")
repo_df = pd.read_parquet("hf://datasets/hao-li/AIDev/repository.parquet")
user_df = pd.read_parquet("hf://datasets/hao-li/AIDev/user.parquet")

pr_comments_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pr_comments.parquet")
pr_reviews_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pr_reviews.parquet")
pr_review_comments_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pr_review_comments.parquet")

pr_commits_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pr_commits.parquet")
pr_commit_details_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pr_commit_details.parquet")

related_issue_df = pd.read_parquet("hf://datasets/hao-li/AIDev/related_issue.parquet")
issue_df = pd.read_parquet("hf://datasets/hao-li/AIDev/issue.parquet")

pr_timeline_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pr_timeline.parquet", columns=None)
pr_task_type_df = pd.read_parquet("hf://datasets/hao-li/AIDev/pr_task_type.parquet")

try:
    human_pr_df = pd.read_parquet("hf://datasets/hao-li/AIDev/human_pull_request.parquet")
    human_pr_task_type_df = pd.read_parquet("hf://datasets/hao-li/AIDev/human_pr_task_type.parquet")
except Exception:
    human_pr_df = pd.DataFrame()
    human_pr_task_type_df = pd.DataFrame()

print("Loaded tables.")

# === Filter to repos with >=100 stars ===
def first_existing(df, candidates, default=None):
    for c in candidates:
        if c in df.columns:
            return c
    return default

repo_key = first_existing(repo_df, ["id", "repo_id", "repository_id"])
stars_col = first_existing(repo_df, ["stargazers_count","stargazers","stars"])
pr_repo_fk = first_existing(pr_df, ["repo_id","repository_id"])

if not all([repo_key, stars_col, pr_repo_fk]):
    raise RuntimeError("Missing join key or stars column.")

repos_100 = repo_df[repo_df[stars_col] >= 100].copy()
prs_100 = pr_df.merge(repos_100[[repo_key, stars_col]], left_on=pr_repo_fk, right_on=repo_key, how="inner")
prs_100 = prs_100.rename(columns={stars_col: "repo_stars"})

PR_ID = "id"
PR_TITLE = "title"
PR_BODY = "body"
PR_URL = "html_url"
PR_NUMBER = first_existing(prs_100, ["number","pr_number"])
PR_CREATED = first_existing(prs_100, ["created_at","created"])
PR_MERGED = first_existing(prs_100, ["merged","merged_at"])
REPO_FULL = first_existing(repo_df, ["full_name","name","repo_name"])

repo_names = repo_df[[repo_key] + ([REPO_FULL] if REPO_FULL else [])].copy()
prs_100 = prs_100.merge(repo_names, left_on=pr_repo_fk, right_on=repo_key, how="left", suffixes=("","_repo"))

# === Security patterns ===
CVE_RE = re.compile(r"\bCVE-\d{4}-\d{4,7}\b", flags=re.IGNORECASE)
CWE_RE = re.compile(r"\bCWE-\d{1,4}\b", flags=re.IGNORECASE)
SEC_TERMS = [
    "vulnerability","vulnerabilities","security","secure","insecure","exploit",
    "xss","cross site scripting","cross-site scripting",
    "sql injection","sqli","rce","remote code execution",
    "ssrf","xxe","csrf","privilege escalation","priv esc","privesc",
    "buffer overflow","overflow","heap overflow","stack overflow",
    "path traversal","directory traversal","traversal",
    "prototype pollution","deserialization","insecure deserialization",
    "cors misconfiguration","open redirect","redirect vulnerability",
    "denial of service","dos","redos","rate limiting",
    "race condition","side-channel","timing attack",
    "auth bypass","authentication bypass","authorization bypass",
    "credential leak","token leak","secret leak","hardcoded secret","api key leak",
    "information disclosure","info leak","sensitive data exposure",
    "weak cipher","weak encryption","insecure default","insecure configuration",
    "mitm","man-in-the-middle",
]
TERM_RE = re.compile(r"|".join([re.escape(t) for t in SEC_TERMS]), flags=re.IGNORECASE)

def find_hits(text):
    if not isinstance(text, str) or not text:
        return set(), set(), []
    cves = set(m.group(0).upper() for m in CVE_RE.finditer(text))
    cwes = set(m.group(0).upper() for m in CWE_RE.finditer(text))
    terms = set(m.group(0) for m in TERM_RE.finditer(text))
    return cves | cwes, terms, []

def snippet(text, regex, max_len=180):
    if not isinstance(text, str) or not text:
        return None
    m = regex.search(text)
    if not m:
        return None
    start = max(m.start()-80, 0)
    end = min(m.end()+100, len(text))
    return text[start:end].replace("\n"," ").strip()

records = []

def push(pr_id, source, text, url=None):
    if not isinstance(text, str) or not text:
        return
    ids, terms, _ = find_hits(text)
    if ids or terms:
        snip = snippet(text, CVE_RE) or snippet(text, TERM_RE)
        records.append({
            "pr_id": pr_id,
            "source": source,
            "matched_ids": ";".join(sorted(ids)) if ids else "",
            "matched_terms": ";".join(sorted(set(t.lower() for t in terms))) if terms else "",
            "snippet": snip if snip else (text[:180].replace("\n"," ") if text else ""),
            "url": url if isinstance(url, str) else "",
        })

# === 1) PR title/body ===
for row in prs_100[[PR_ID, PR_TITLE, PR_BODY, PR_URL]].itertuples(index=False, name=None):
    pr_id, title, body, url = row
    push(pr_id, "pr_title", title, url)
    push(pr_id, "pr_body", body, url)

# === 2) PR comments ===
if not pr_comments_df.empty:
    prc_pid = first_existing(pr_comments_df, ["pr_id","pull_request_id","issue_id"])
    prc_body = first_existing(pr_comments_df, ["body","comment_body","text"])
    prc_url  = first_existing(pr_comments_df, ["html_url","url"])
    if prc_pid and prc_body:
        for row in pr_comments_df[[prc_pid, prc_body] + ([prc_url] if prc_url else [])].itertuples(index=False, name=None):
            if len(row) >= 2:
                pid, body = row[0], row[1]
                url = row[2] if len(row) > 2 else ""
                push(pid, "pr_comment", body, url)

# === 3) Review comments ===
if not pr_review_comments_df.empty:
    prrc_pid = first_existing(pr_review_comments_df, ["pr_id","pull_request_id","review_id","pull_request_review_id"])
    prrc_body = first_existing(pr_review_comments_df, ["body","comment_body","text"])
    prrc_url  = first_existing(pr_review_comments_df, ["html_url","url"])
    if prrc_pid and prrc_body:
        for row in pr_review_comments_df[[prrc_pid, prrc_body] + ([prrc_url] if prrc_url else [])].itertuples(index=False, name=None):
            if len(row) >= 2:
                pid, body = row[0], row[1]
                url = row[2] if len(row) > 2 else ""
                push(pid, "pr_review_comment", body, url)

# === 4) Reviews ===
if not pr_reviews_df.empty:
    prr_pid = first_existing(pr_reviews_df, ["pr_id","pull_request_id"])
    prr_body = first_existing(pr_reviews_df, ["body","text"])
    if prr_pid and prr_body:
        for row in pr_reviews_df[[prr_pid, prr_body]].itertuples(index=False, name=None):
            if len(row) >= 2:
                pid, body = row[0], row[1]
                push(pid, "pr_review", body, "")

# === 5) Commits ===
commit_sha_col = first_existing(pr_commits_df, ["sha","commit_sha"])
prc_pr_col = first_existing(pr_commits_df, ["pr_id","pull_request_id"])
det_sha_col = first_existing(pr_commit_details_df, ["sha","commit_sha"])
det_msg_col = first_existing(pr_commit_details_df, ["message","commit_message"])
if prc_pr_col and commit_sha_col and det_sha_col and det_msg_col:
    sha_to_msg = pr_commit_details_df[[det_sha_col, det_msg_col]].dropna().drop_duplicates()
    sha_to_msg = sha_to_msg.rename(columns={det_sha_col: "sha", det_msg_col: "message"})
    pr_to_sha = pr_commits_df[[prc_pr_col, commit_sha_col]].dropna().rename(columns={prc_pr_col:"pr_id", commit_sha_col:"sha"})
    pr_commit_msgs = pr_to_sha.merge(sha_to_msg, on="sha", how="left")
    for row in pr_commit_msgs[["pr_id","message"]].itertuples(index=False, name=None):
        if len(row) >= 2:
            pid, msg = row[0], row[1]
            push(pid, "commit_message", msg, "")

# === 6) Related issues ===
ri_pr = first_existing(related_issue_df, ["pr_id","pull_request_id"])
ri_issue = first_existing(related_issue_df, ["issue_id","id"])
issue_id = first_existing(issue_df, ["id","issue_id"])
issue_title = first_existing(issue_df, ["title"])
issue_body = first_existing(issue_df, ["body","description"])
issue_url = first_existing(issue_df, ["html_url","url"])
if ri_pr and ri_issue and issue_id and (issue_title or issue_body):
    ri = related_issue_df[[ri_pr, ri_issue]].dropna().rename(columns={ri_pr:"pr_id", ri_issue:"issue_id"})
    issues = issue_df[[issue_id] + [c for c in [issue_title, issue_body, issue_url] if c]].rename(
        columns={issue_id:"issue_id", issue_title or "": "issue_title", issue_body or "": "issue_body", issue_url or "": "issue_url"}
    )
    pr_issues = ri.merge(issues, on="issue_id", how="left")
    for row in pr_issues.itertuples(index=False, name=None):
        if len(row) >= 4:
            pid, iid, title, body = row[0], row[1], row[2], row[3]
            url = row[4] if len(row) > 4 else ""
            push(pid, "related_issue_title", title, url)
            push(pid, "related_issue_body", body, url)

# === 7) Labels ===
if not pr_timeline_df.empty:
    ev_type_col = first_existing(pr_timeline_df, ["event","type","event_type"])
    ev_pr_col = first_existing(pr_timeline_df, ["pr_id","pull_request_id","issue_id"])
    label_col = first_existing(pr_timeline_df, ["label","label_name","name"])
    if ev_type_col and ev_pr_col and label_col:
        ev = pr_timeline_df[pr_timeline_df[ev_type_col].astype(str).str.lower().eq("labeled")]
        if not ev.empty:
            ev = ev[[ev_pr_col, label_col]].rename(columns={ev_pr_col:"pr_id", label_col:"label"}).dropna()
            for _, (pid, lab) in ev.iterrows():
                push(pid, "label", str(lab), "")

# === Aggregate per PR ===
evidence_df = pd.DataFrame(records)

# --- Safety Nets ---
evidence_df = evidence_df[evidence_df["pr_id"].notna()]
if PR_ID in prs_100.columns:
    valid_pr_ids = prs_100[PR_ID].unique()
    evidence_df = evidence_df[evidence_df["pr_id"].isin(valid_pr_ids)]

if evidence_df.empty:
    print("No explicit security/vuln mentions found.")
    out = pd.DataFrame(columns=[
        "pr_id","repo_full_name","pr_number","pr_url","created_at","merged_at","repo_stars",
        "matched_ids","matched_terms","evidence_sources","evidence_snippets","evidence_count"
    ])
else:
    agg = (
        evidence_df.groupby("pr_id", as_index=False)
        .agg({
            "matched_ids": lambda s: ";".join(sorted(set(";".join(s).split(";")) - {""})),
            "matched_terms": lambda s: ";".join(sorted(set(";".join(s).split(";")) - {""})),
            "source": lambda s: ";".join(sorted(set(s))),
            "snippet": lambda s: " || ".join([x for x in s if isinstance(x,str) and x][:5])
        })
        .rename(columns={"source":"evidence_sources","snippet":"evidence_snippets"})
    )
    agg["evidence_count"] = agg["evidence_sources"].apply(lambda x: len(x.split(";")) if isinstance(x,str) and x else 0)

    meta_cols = {
        "repo_full_name": REPO_FULL,
        "pr_number": PR_NUMBER,
        "pr_url": PR_URL,
        "created_at": PR_CREATED,
        "merged_at": PR_MERGED,
        "repo_stars": "repo_stars",
        "pr_title": PR_TITLE,
    }
    meta_keep = [PR_ID] + [c for c in meta_cols.values() if c and c in prs_100.columns]
    meta = prs_100[meta_keep].copy()
    rename_map = {PR_ID:"pr_id"}
    for out_name, src in meta_cols.items():
        if src and src in prs_100.columns:
            rename_map[src] = out_name
    meta = meta.rename(columns=rename_map)

    out = agg.merge(meta, on="pr_id", how="left")
    out = out.drop_duplicates(subset=["pr_id"])

    front = ["pr_id","repo_full_name","pr_number","pr_title","pr_url","created_at","merged_at","repo_stars",
             "matched_ids","matched_terms","evidence_sources","evidence_snippets","evidence_count"]
    front = [c for c in front if c in out.columns]
    out = out[front + [c for c in out.columns if c not in front]]

# === Write outputs (next to this script) ===
output_dir = os.path.dirname(os.path.abspath(__file__))
security_csv = os.path.join(output_dir, "security_prs.csv")
summary_csv = os.path.join(output_dir, "security_prs_summary.csv")

try:
    out.to_csv(security_csv, index=False)
    summary = {
        "total_repos_>=100_stars": int(repos_100.shape[0]),
        "total_prs_in_those_repos": int(prs_100.shape[0]),
        "total_security_prs": int(out.drop_duplicates(subset=["pr_id"]).shape[0]),
        "prs_with_cve_id": int((out["matched_ids"].fillna('').str.contains("CVE-", case=False)).sum()),
        "prs_with_cwe_id": int((out["matched_ids"].fillna('').str.contains("CWE-", case=False)).sum())
    }
    pd.DataFrame([summary]).to_csv(summary_csv, index=False)

    print("\n=== Summary ===")
    for k, v in summary.items():
        print(f"{k}: {v}")

    print(f"\n✅ security_prs.csv saved to: {security_csv}")
    print(f"✅ security_prs_summary.csv saved to: {summary_csv}")

except Exception as e:
    print(f"\n❌ Error writing CSV files: {e}")
